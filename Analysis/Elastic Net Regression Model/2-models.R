# After determining the non redundant variables, please build elastic net regression models with different hyperparameter ranges -tuneLength
# # Run this script with the command - source("xxxxxx/2-models", echo=TRUE, max.deparse.length=300000)
library(Hmisc)
library(dplyr)
library(nlme)                    # Fit Gaussian linear and nonlinear mixed-effects models
library(lme4)                    # Fit linear and generalized linear mixed-effects models
library(lattice)                 # Data visualization system
library(optimx)
library(car)
library(glmnet)
library(caret)
library(BradleyTerry2)
library(forcats)
library(doMC)  

# multi cores
registerDoMC(cores=10)

Sys.setenv('R_MAX_VSIZE'=31000000000)
options(max.print=99999)


ext <- readxl::read_xlsx("xxxxxx/metricExtend.xlsx")
encoded_category <- model.matrix(~category-1, data=ext)
ext <- cbind(ext,encoded_category)
ext <- data.matrix(ext)

ext <- transform(ext, size = (size))
ext <- transform(ext, users = (users))
ext <- transform(ext, rate = (rate))
ext <- transform(ext, rateCount = (rateCount))

f_all <- ext
f_all$testBase <- NULL
f_all$'financial.and.payment.information'<- NULL 
f_all$'health.information'<- NULL
f_all$cluster <- NULL

f_all <- select(f_all, -contains("Files_."))
f_all<- select(f_all, -contains("Number_."))
f_all$category <- NULL


names(f_all)[names(f_all) == 'login'] <- 'isLogin'
names(f_all)[names(f_all) == 'grant'] <- 'isGrant'
names(f_all)[names(f_all) == 'inactive'] <- 'isInactive'
names(f_all)[names(f_all) == 'fullInactive'] <- 'isFullyInactive'

minMax <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
f_all <- as.data.frame(lapply(f_all, minMax))
f_all <- f_all %>% select_if(~ !any(is.na(.)))


f_all <- f_all[!duplicated(f_all), ]
f_all <- f_all[vapply(f_all, function(x) length(unique(x)) >= 1, logical(1L))]

# Match the variable names with the research metric names
metrics <- c(CountClassBase='IFANIN',CountClassDerived='NOC',CountClassCoupled='CBO',CountDeclClassVariable='NV',
             CountDeclInstanceMethod='NIM',CountDeclInstanceVariable='NIV',CountDeclMethod='WMC',CountDeclMethodAll='RFC',
             CountDeclMethodFriend='NFM',CountDeclMethodPrivate='NPRM',CountDeclMethodPublic='NPM',CountInput='FANIN',
             CountLine='NL',CountLineBlank='BLOC',CountLineCode='LOC',CountLineComment='CLOC',
             CountOutput='FANOUT',CountPath='NPATH',Cyclomatic='CC',CyclomaticModified='CC3',
             CyclomaticStrict='CC2',Essential='ev(G)',MaxInheritanceTree='DIT',PercentLackOfCohesion='LCOM',
             SumCyclomatic='WMC')

f_new <- data.frame()
for (metr in names(metrics)){
  if(metr %in% colnames(f_all)){
    if(nrow(f_new)!=0){f_new <- cbind(f_all[metr],f_new)}
    else{f_new <- f_all[metr]}
  }
}
colnames(f_new) <- paste(colnames(f_new),metrics[colnames(f_new)],sep="-") 

f_new <- cbind(f_all[,49:ncol(f_all)],f_new)

# X and Y datasets + normalization
# x : factors
x <- f_new %>% select(-time) %>% select(-loading) %>% select(-static) 
# y : performance metrics - page load time
y <- f_new %>% select(time)


# Model Building : Elastic Net Regression 
control <- trainControl(method = "adaptive_cv", #repeatedcv, adaptive_cv
                        number = 100, 
                        repeats = 5, #5 separate if k-fold cross-validations are used.
                        allowParallel = TRUE,
                        adaptive = list(min = 100, # is the minimum number of resamples that will be used for each tuning parameter. The default value is 5 and increasing it will decrease the speed-up generated by adaptive resampling but should also increase the likelihood of finding a good model.
                                        alpha = 0.05, # Confidence level for removing hyperparameter settings. To date, this value has not shown much of an effect.
                                        method = "BT",# BT: Bradly-Terry Resampling method (here you can instead also use "gls"). BT may be more useful when you expect the model to do very well (e.g. an area under the ROC curve near 1) or when there are a large number of tuning parameter settings.
                                        complete = FALSE),# a logical value that specifies whether train should generate the full resampling set if it finds an optimal solution before the end of resampling. If you want to know the optimal parameter settings and donâ€™t care much for the estimated performance value, a value of FALSE would be appropriate here.
                        search = "random",
                        verboseIter = FALSE)

elastic_model <- function(i,x,y){
  tryCatch({
    out_glmnet <- list(train(time ~ .,
                             data = cbind(x, y),
                             method = "glmnet",
                             family = "gaussian",
                             standardize = FALSE,
                             tuneLength = i, # x trys in Fold000.rep1 total: i*number
                             um.threads = 10,
                             trControl = control))
    
    message(paste("tuneLength=",i, "is applicable"))
    return(out_glmnet)
  },
  error=function(e) {
    message(paste("tuneLength=",i, "does not apply"))
    # Choose a return value in case of error
    return(NULL)
  })
}

con <- file("xxxxxx/modelLogT.txt")
sink(con, append=TRUE)
sink(con, append=TRUE, type="message")

# Training ELastic Net Regression model 
modelt <- list()
for (i in seq(1,300, by=1)){
  modelt[i] <- elastic_model(i=i,x=x,y=y)
}
sink()
sink(type="message")
closeAllConnections()
saveRDS(modelt,'xxxxxx/page load time.rds')
modelt <- NULL

y <- f_new %>% select(loading)
elastic_model <- function(i,x,y){
  tryCatch({
    out_glmnet <- list(train(static ~ .,
                             data = cbind(x, y),
                             method = "glmnet",
                             family = "gaussian",
                             standardize = FALSE,
                             tuneLength = i, # x trys in Fold000.rep1 total: i*number
                             um.threads = 10,
                             trControl = control))
    
    message(paste("tuneLength=",i, "is applicable"))
    return(out_glmnet)
  },
  error=function(e) {
    message(paste("tuneLength=",i, "does not apply"))
    # Choose a return value in case of error
    return(NULL)
  })
}

con <- file("/Users/jinbihui/Downloads/ComboLogP.txt")
sink(con, append=TRUE)
sink(con, append=TRUE, type="message")
# Training ELastic Net Regression model 
modelp <- list()
for (i in seq(1,300, by=1)){
  modelp[i] <- elastic_model(i=i,x=x,y=y)
}
sink()
sink(type="message")
saveRDS(modelp,'xxxxxx/page load energy consumption.rds')
modelp <- NULL


y <- f_new %>% select(static)
elastic_model <- function(i,x,y){
  tryCatch({
    out_glmnet <- list(train(static ~ .,
                             data = cbind(x, y),
                             method = "glmnet",
                             family = "gaussian",
                             standardize = FALSE,
                             tuneLength = i, # x trys in Fold000.rep1 total: i*number
                             um.threads = 10,
                             trControl = control))
    
    message(paste("tuneLength=",i, "is applicable"))
    return(out_glmnet)
  },
  error=function(e) {
    message(paste("tuneLength=",i, "does not apply"))
    # Choose a return value in case of error
    return(NULL)
  })
}

con <- file("/Users/jinbihui/Downloads/ComboLogS.txt")
sink(con, append=TRUE)
sink(con, append=TRUE, type="message")
# Training ELastic Net Regression model 
models <- list()
for (i in seq(1,300, by=1)){
  models[i] <- elastic_model(i=i,x=x,y=y)
}
sink()
sink(type="message")
closeAllConnections()
saveRDS(modelt,'xxxxxx/stabilized energy consumption.rds')


registerDoMC(cores=1)